#!/usr/bin/env python3
"""
Ion Analytics Spider Runner
Demonstrates how to run the Ion Analytics spider with the existing infrastructure
"""

import os
import sys
import subprocess
from datetime import datetime

def run_spider_with_scrapy():
    """Run the spider using Scrapy's command line interface"""
    print("Starting Ion Analytics spider with Scrapy...")
    print(f"Timestamp: {datetime.now().isoformat()}")
    
    # Change to the correct directory
    os.chdir('/home/lij/dev/projects/MergerTracker/backend')
    
    # Run the spider with Scrapy CLI
    cmd = [
        'scrapy', 'crawl', 'ion_analytics',
        '-s', 'LOG_LEVEL=INFO',
        '-s', 'ROBOTSTXT_OBEY=True',
        '-s', 'DOWNLOAD_DELAY=4',
        '-o', f'ion_analytics_output_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    ]
    
    print(f"Command: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        print("\n" + "="*60)
        print("SPIDER EXECUTION RESULT")
        print("="*60)
        print(f"Return code: {result.returncode}")
        
        if result.stdout:
            print("\nSTDOUT:")
            print(result.stdout)
        
        if result.stderr:
            print("\nSTDERR:")
            print(result.stderr)
            
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print("‚ùå Spider execution timed out (5 minutes)")
        return False
    except FileNotFoundError:
        print("‚ùå Scrapy not found. Please install scrapy first.")
        return False
    except Exception as e:
        print(f"‚ùå Error running spider: {e}")
        return False

def run_spider_programmatically():
    """Run the spider programmatically using Scrapy's API"""
    print("Starting Ion Analytics spider programmatically...")
    
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        from scraper.spiders.ion_analytics_spider import IonAnalyticsSpider
        
        # Get project settings
        settings = get_project_settings()
        
        # Override some settings for testing
        settings.update({
            'LOG_LEVEL': 'INFO',
            'ROBOTSTXT_OBEY': True,
            'DOWNLOAD_DELAY': 4,
            'CONCURRENT_REQUESTS_PER_DOMAIN': 2,
            'FEEDS': {
                f'ion_analytics_output_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json': {
                    'format': 'json',
                    'encoding': 'utf8',
                },
            },
        })
        
        # Create and start the crawler
        process = CrawlerProcess(settings)
        process.crawl(IonAnalyticsSpider)
        process.start()  # Blocks until finished
        
        print("‚úÖ Spider execution completed successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Error running spider programmatically: {e}")
        return False

def validate_output_files():
    """Validate the output files generated by the spider"""
    import glob
    import json
    
    output_files = glob.glob('ion_analytics_output_*.json')
    
    if not output_files:
        print("‚ùå No output files found")
        return False
    
    latest_file = max(output_files, key=os.path.getctime)
    print(f"üìÑ Latest output file: {latest_file}")
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"‚úÖ Output file is valid JSON with {len(data)} items")
        
        # Analyze the scraped data
        if data:
            sample_item = data[0]
            print(f"üìä Sample item fields: {list(sample_item.keys())}")
            
            # Count item types
            news_items = sum(1 for item in data if 'title' in item and 'content' in item)
            deal_items = sum(1 for item in data if 'deal_type' in item)
            
            print(f"üìà Data breakdown:")
            print(f"  - News articles: {news_items}")
            print(f"  - Deal items: {deal_items}")
            print(f"  - Total items: {len(data)}")
        
        return True
        
    except json.JSONDecodeError as e:
        print(f"‚ùå Invalid JSON in output file: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Error validating output: {e}")
        return False

def main():
    """Main execution function"""
    print("="*70)
    print("ION ANALYTICS SPIDER RUNNER")
    print("="*70)
    
    # Check if we're in the right directory
    if not os.path.exists('scraper'):
        print("‚ùå Please run this script from the backend directory")
        sys.exit(1)
    
    # Try to run the spider
    print("\n1. Attempting to run spider with Scrapy CLI...")
    success_cli = run_spider_with_scrapy()
    
    if not success_cli:
        print("\n2. Attempting to run spider programmatically...")
        success_prog = run_spider_programmatically()
        
        if not success_prog:
            print("‚ùå Both execution methods failed")
            sys.exit(1)
    
    # Validate output
    print("\n3. Validating output files...")
    success_output = validate_output_files()
    
    if success_output:
        print("\nüéâ Ion Analytics spider executed successfully!")
        print("\nNext steps:")
        print("1. Review the output JSON file for scraped data")
        print("2. Integrate with your data pipeline")
        print("3. Set up scheduled execution")
        print("4. Monitor for rate limiting and errors")
    else:
        print("\n‚ö†Ô∏è Spider ran but output validation failed")
        print("Check the logs for potential issues")

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ùå Execution interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå Unexpected error: {e}")
        sys.exit(1)